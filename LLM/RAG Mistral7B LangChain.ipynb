{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation) con Mistral\n",
    "\n",
    "Adaptado y editado por **Etson Ronaldao Rojas Cahuana**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La generación aumentada por recuperación (RAG) es un marco de IA que sinergiza las capacidades de los modelos de lenguaje de gran escala (LLMs) y los sistemas de recuperación de información. Es útil para responder preguntas o generar contenido aprovechando el conocimiento externo. \n",
    "Hay dos pasos principales en RAG: \n",
    "1) recuperación: recuperar información relevante de una base de conocimientos con incrustaciones de texto almacenadas en una base de datos de vectores. \n",
    "2) generación: insertar la información relevante en el prompt para que el LLM genere información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG con Mistral y LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargar el modelo de: https://mistral.ai/news/announcing-mistral-7b/ \n",
    "\n",
    "- mistral-7b-v0.1.tar \n",
    "- 13.5GB\n",
    "\n",
    "O descargarlo directamente desde HuggingFace atraves de un token de acceso gratuito:\n",
    "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1 en este cuaderno usaremos este metodo.\n",
    "\n",
    "Instalar librerias\n",
    "```python\n",
    "!pip install gradio --quiet \n",
    "!pip install xformer --quiet \n",
    "!pip install chromadb --quiet \n",
    "!pip install langchain==0.2.6 --quiet \n",
    "!pip install accelerate --quiet \n",
    "!pip install transformers --quiet\n",
    "!pip install bitsandbytes --quiet \n",
    "!pip install unstructured --quiet\n",
    "!pip install -u sentence-transformers --quiet \n",
    "!pip install langchain-community langchain-core \n",
    "!pip install huggingface_hub \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Importa la biblioteca PyTorch para machine learning.\n",
    "#import gradio as gr  # Importa Gradio, una biblioteca para crear interfaces de usuario para modelos de aprendizaje automático.\n",
    "from textwrap import fill  # Importa la función fill de textwrap para envolver texto en líneas de ancho fijo.\n",
    "from IPython.display import Markdown, display  # Importa funciones de IPython para mostrar texto en formato Markdown.\n",
    "\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate # python vesrion 3.12.3 !pip install -U pydantic pydantic_core \n",
    "from langchain import PromptTemplate  # Importa la clase PromptTemplate de la biblioteca langchain.\n",
    "from langchain import HuggingFacePipeline  # Importa el pipeline de HuggingFace para procesamiento de lenguaje natural.\n",
    "from langchain.vectorstores import Chroma  # Importa el almacenamiento de vectores Chroma de langchain.\n",
    "from langchain.schema import AIMessage, HumanMessage  # Importa clases de esquema para mensajes AI y humanos.\n",
    "from langchain.memory import ConversationBufferMemory  # Importa la memoria de buffer de conversación de langchain.\n",
    "from langchain.embeddings import HuggingFaceEmbeddings  # Importa los embeddings de HuggingFace para procesamiento de texto.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Importa el divisor de texto de caracteres recursivo de langchain.\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader  # Importa cargadores de documentos sin estructura de langchain.\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
    "\n",
    "from transformers import  BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline # Importa configuraciones, modelos y pipelines de la biblioteca Transformers.\n",
    "#pip install transformers[sklearn] --force-reinstall\n",
    "import warnings  # Importa el módulo warnings para manejar advertencias.\n",
    "warnings.filterwarnings('ignore')  # Ignora las advertencias durante la ejecución.\n",
    "\n",
    "from huggingface_hub import notebook_login  # Importa la función notebook_login de huggingface_hub para iniciar sesión desde un cuaderno. !pip install ipywidgets\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuracion de nuestro entorno, es necesario una GPU para acelerar con los experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch                    :2.3.0\n",
      "GPU Available            :True\n",
      "Cuda Built               :1\n",
      "Name of GPU              :NVIDIA GeForce RTX 3070 Ti Laptop GPU\n",
      "Memory Usage:\n",
      "                         Allocated 0.0 GB\n",
      "                         Cached    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch\".ljust(25) + f\":{torch.__version__}\")\n",
    "print(\"GPU Available\".ljust(25) + f\":{torch.cuda.is_available()}\")\n",
    "print(\"Cuda Built\".ljust(25) + f\":{torch.cuda.device_count()}\")\n",
    "print(\"Name of GPU\".ljust(25) + f\":{torch.cuda.get_device_name(0)}\")\n",
    "print('Memory Usage:')\n",
    "print(\"\".ljust(25)+'Allocated', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print(\"\".ljust(25)+'Cached   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener acceso al repositorio de HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1026898a124949ce8eefaea970e783d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()\n",
    "#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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especificar el modelo a utilizar en este caso ``Mistral-7B-Instruct-v0.1`` con direccion de huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos PyTorch utilizan puntos flotantes de 32 bits, lo que significa que un solo parámetro ocupa 32 \"bits\" en la memoria de la GPU. La cuantificación tiene como objetivo reemplazar estos parámetros con puntos flotantes de 16 bits, enteros de 8 bits o incluso enteros de 4 bits. Una cuantificación exitosa conduce a mejoras dramáticas en la velocidad computacional y reducciones en el uso de memoria, lo que significa que los modelos grandes se pueden ejecutar en GPU de gama baja, chips gráficos integrados o incluso CPU.\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:500/1*hWIaIAQ7GWbrjfbaoUoYxw.jpeg'>\n",
    "\n",
    "Fuente: \n",
    "- https://pub.towardsai.net/llm-quantisation-quantise-hugging-face-model-with-gptq-awq-and-bitsandbytes-a4ad45cd8b48\n",
    "- https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "# Se configura la cuantización utilizando BitsAndBytesConfig:\n",
    "# - load_in_4bit=True: Cuantización de 4 bits.\n",
    "# - bnb_4bit_compute_dtype=torch.float16: Define el tipo de datos para los cálculos como float16.\n",
    "# - bnb_4bit_quant_type=\"nf4\": Normalize float4.\n",
    "# - bnb_4bit_use_double_quant=True: Se habilita la segunda cuantizacion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``tokenizer`` proporciona una interfaz sencilla para cargar el tokenizador adecuado basado en el nombre del modelo ``MODEL_NAME``. El tokenizer se encarga de convertir texto en tokens (unidades de texto más pequeñas) que el modelo puede procesar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809c5fc7b50d442bb9edcc5b1f1d1afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986ace46d4884f128a463a65e5c6d3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d95ae0a35684cfa8cc5558289944b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4f639068db4c6393f2408ac135d50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "# Se carga el tokenizer utilizando el nombre del modelo especificado anteriormente.\n",
    "# - use_fast=True: Habilita el uso de la versión rápida del tokenizer si está disponible.\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "# Se ajusta el token de relleno del tokenizer al token de fin de secuencia (EOS, end of sequence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ``AutoModelForCausalLM`` forma parte de la clase ``transformers`` e instancia una clase de modelo en función de una configuración proporcionada.\n",
    "- ``from_pretrained`` devuelve una instancia del modelo preentrenado correspondiente según la configuración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfef6db857e54057a7d9f4a8509ddf87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1529bbfc670a4ca293714ef17227dc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b9a9002b1d4bc6b472b963fad9a32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc35ea473a88433e8618e78898cec4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6438741a1804468eb838e3719c8d1f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fd2cbc0ddc4117ade0d7f1911ffc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c9450da3994ac4aff903fbfd74085b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "# Se carga un modelo de lenguaje causal utilizando AutoModelForCausalLM.from_pretrained().\n",
    "# - MODEL_NAME: Especifica el nombre del modelo preentrenado que se va a cargar.\n",
    "# - torch_dtype=torch.float16: Indica el tipo de datos de PyTorch que se utilizará para los cálculos, en este caso, float16 para cuantización.\n",
    "# - trust_remote_code=True: Confirma la confianza en el código remoto (por ejemplo, del hub de modelos de Hugging Face).\n",
    "# - device_map=\"auto\": Asigna automáticamente dispositivos (por ejemplo, GPU) disponibles para ejecución.\n",
    "# - quantization_config=quantization_config: Aplica la configuración de cuantización previamente definida al modelo cargado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparametros de Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con ``GenerationConfig.from_pretrained`` se carga una configuración preentrenada para la generación de texto desde un modelo específico identificado por ``MODEL_NAME``. Esta configuración incluye parámetros predefinidos como la temperatura (temperature), la probabilidad de muestreo (top_p), entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME) \n",
    "\n",
    "#Establece el máximo número de nuevos tokens que el modelo puede generar en una sola llamada de generación.\n",
    "generation_config.max_new_tokens = 1024 \n",
    "#Ajusta la temperatura de muestreo para controlar la creatividad de la generación de texto. \n",
    "#Una temperatura baja tiende a producir predicciones más determinísticas.\n",
    "generation_config.temperature = 0.0001 \n",
    "#Establece la probabilidad acumulativa máxima para la selección de tokens durante el muestreo con la técnica de (top_p).\n",
    "generation_config.top_p = 1 #0.95 \n",
    "#Habilita el muestreo estocástico durante la generación de texto \n",
    "#lo cual permite que el modelo explore diferentes opciones en lugar de generar siempre la salida más probable.\n",
    "generation_config.do_sample = True \n",
    "#Aplica un factor de penalización para reducir la probabilidad de que el modelo repita secuencias de tokens durante la generación.\n",
    "generation_config.repetition_penalty = 1.15 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ``pipeline`` permite crear de manera sencilla un objeto que encapsula la funcionalidad de un modelo preentrenado para realizar una tarea específica de NLP, en este caso ``text-generation``.\n",
    "\n",
    "El ``token_pad``, a menudo abreviado como ``PAD``, es un token especial que se utiliza para estandarizar la longitud de las secuencias de entrada durante el entrenamiento. En el ajuste, los modelos de lenguaje se entrenan en lotes de datos, y estos lotes suelen constar de secuencias de diferentes longitudes. Para procesar eficientemente estas secuencias, deben tener la misma longitud. Aquí es donde entra en juego ``token_pad``.\n",
    "\n",
    "El ``eos_token``, denotado como ``EOS`` u otra etiqueta similar, sirve como una señal para el modelo de que una secuencia ha llegado a su conclusión. Indica el punto de terminación de una secuencia y ayuda al modelo a comprender los límites entre diferentes fragmentos de texto. En las tareas de generación de lenguaje natural, el token de fin de secuencia guía al modelo para producir una salida coherente y bien estructurada.\n",
    "\n",
    "Mas informacion en: https://www.natebrake.com/blog/llm/end-of-sequence-explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    return_full_text = True, #Indica que se desea obtener el texto completo generado en lugar de segmentos más pequeños.\n",
    "    generation_config = generation_config, #Carcar la configuracfion de hiperparametros\n",
    "    pad_token_id = tokenizer.eos_token_id # Se ajusta el token de relleno del tokenizer al token de fin de secuencia (EOS, end of sequence).\n",
    "                                          # Se evita la advertencia Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lograremos encapsular el modelo en ``llm`` con ``HuggingFacePipeline`` que nos proporciona una interfaz conveniente para utilizar pipelines preentrenados de Hugging Face, permitiendo realizar tareas como generación de texto, resumen de texto, traducción, entre otras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(\n",
    "    pipeline = pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el modelo base generando consultas para que nos de informacion de algun tema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<p>Explica acerca de los tipos de investigación cientifica en un par de oraciones.\n",
       "\n",
       "La investigación científica se puede dividir en dos categorías principales: la investigación fundamental y la investigación aplicada. La investigación fundamental es una exploración del conocimiento por su propio interés, mientras que la investigación aplicada tiene como objetivo resolver problemas reales o crear productos útiles para el ser humano. Además, existen varios subtipos de investigación científica, incluyendo la observación, la experimentación, la teoría y la simulación.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Es importante indicar en el query que genere algo corto por que puede tardar mas tiempo en generar la respuesta.\n",
    "query = \"Explica acerca de los tipos de investigación cientifica en un par de oraciones.\"  \n",
    "result = llm(query)\n",
    "\n",
    "display(Markdown(f\"<p>{result}</p>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<p>Explica la diferencia entre ChatGPT y los LLM de código abierto en un par de líneas.\n",
       "\n",
       "ChatGPT es una herramienta de generación de texto creada por OpenAI, que utiliza el modelo de lenguaje propietario de la empresa para crear respuestas a preguntas en tiempo real. Los LLMs de código abierto son modelos de aprendizaje profundo que han sido entrenados con código fuente públicamente disponible y pueden ser utilizados para generar código o analizar código existente.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Explica la diferencia entre ChatGPT y los LLM de código abierto en un par de líneas.\"\n",
    "result = llm(query)\n",
    "\n",
    "display(Markdown(f\"<p>{result}</p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar texto usando un template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un prompt es una entrada cuidadosamente elaborada que se le da a un modelo de lenguaje para obtener una respuesta deseada específica. Los prompt son, en esencia, instrucciones o preguntas diseñadas para guiar la salida del modelo en una dirección determinada. \n",
    "\n",
    "``PromptTemplate`` nos permite personalizar nuestras consultas o preguntas, en general esta compuesto por:\n",
    "- **Instrucciones**: Informa al modelo sobre la tarea general en cuestión y cómo abordarla, como utilizar la información externa proporcionada, procesar consultas y estructurar la salida. Esta sección suele ser constante dentro de una plantilla de prompt. Un caso de uso común implica decirle al modelo \"Eres un asistente útil de XX\", motivándolo a tomar su rol más seriamente.\n",
    "\n",
    "- **Contexto**: Actúa como una fuente adicional de conocimiento para el modelo. Esta información puede ser insertada manualmente en el prompt, recuperada a través de búsquedas en bases de datos vectoriales o traída mediante otros métodos (como llamadas a APIs, uso de calculadoras, etc.). \n",
    "\n",
    "- **Prompt ingresado**: Representa la pregunta o tarea específica para que el modelo grande la aborde. Esta parte podría fusionarse con la sección de \"Instrucciones\", pero separarla en un componente independiente organiza mejor la estructura y facilita la reutilización de la plantilla. Normalmente se pasa como una variable a la plantilla del prompt antes de invocar al modelo para formar un prompt específico.\n",
    "\n",
    "- **Indicador de salida**: Marca el inicio del texto a ser generado. Es similar a escribir \"Solución:\" en un examen de matemáticas en la infancia, señalando el comienzo de tu respuesta. Si se genera código Python, la palabra \"import\" puede usarse para indicar al modelo que comience a escribir código Python (ya que la mayoría de los scripts de Python comienzan con importaciones). Esta parte a menudo es superflua al conversar con ChatGPT, pero en LangChain, los agentes a menudo utilizan un \"Pensamiento:\" como introducción, indicando al modelo que comience a generar su razonamiento.\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:512/1*C-VbyYG5Iukrpc_3VbHvhw.png'>\n",
    "\n",
    "Fuente:\n",
    "- https://tonylixu.medium.com/langchain-prompt-template-0359d96090c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<b>Instrucciones: </b>\n",
    "<em> Utiliza los siguientes fragmentos de contexto para responder la pregunta al final.\n",
    "Si no sabes la respuesta, simplemente di que no lo sabes, no intentes inventar una respuesta.\n",
    "Utiliza un máximo de tres oraciones y mantén la respuesta lo más concisa posible.\n",
    "Agrega \"¡Gracias por preguntar!\" al final de la respuesta. </em>\n",
    "\n",
    "<b>Consulta: {text}</b>\n",
    "\n",
    "<b>Respuesta: </b>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"text\"],\n",
    "    template = template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<p><b>Instrucciones: </b>\n",
       "<em> Utiliza los siguientes fragmentos de contexto para responder la pregunta al final.\n",
       "Si no sabes la respuesta, simplemente di que no lo sabes, no intentes inventar una respuesta.\n",
       "Utiliza un máximo de tres oraciones y mantén la respuesta lo más concisa posible.\n",
       "Agrega \"¡Gracias por preguntar!\" al final de la respuesta. </em>\n",
       "\n",
       "<b>Consulta: Explica acerca de los tipos de investigación cientifica</b>\n",
       "\n",
       "<b>Respuesta: </b>\n",
       "Los tipos de investigación científica son diversas y se clasifican según su objetivo, método y ámbito. Hay cuatro tipos principales: observacional, experimental, teórica y aplicada. ¡Gracias por preguntar!</p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Explica acerca de los tipos de investigación cientifica\"\n",
    "result = llm(prompt.format(text=query))\n",
    "\n",
    "# Mostrar informacion completa de la consulta\n",
    "display(Markdown(f\"<p>{result}</p>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<p><b>Instrucciones: </b>\n",
       "<em> Utiliza los siguientes fragmentos de contexto para responder la pregunta al final.\n",
       "Si no sabes la respuesta, simplemente di que no lo sabes, no intentes inventar una respuesta.\n",
       "Utiliza un máximo de tres oraciones y mantén la respuesta lo más concisa posible.\n",
       "Agrega \"¡Gracias por preguntar!\" al final de la respuesta. </em>\n",
       "\n",
       "<b>Consulta: Quienes son los autores?\n",
       " UNIVERSIDAD NACIONAL DE SAN ANTONIO ABAD DEL CUSCO \n",
       " \n",
       "FACULTAD DE INGENIERÍA ELÉCTRICA, ELECTRÓNICA, INFORMÁTICA Y \n",
       "MECÁNICA \n",
       "ESCUELA PROFESIONAL DE INGENIERÍA INFORMÁTICA Y DE SISTEMAS \n",
       " \n",
       "TESIS \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       "PRESENTADO POR: \n",
       "Br. VICTOR ABEL CHOQUEVILCA QUISPE \n",
       " \n",
       "Br. ERIKA ALEXANDRA MORALES VALENCIA  \n",
       " \n",
       "PARA OPTAR EL TITULO PROFESIONAL DE \n",
       "INGENIERO INFORMÁTICO Y DE SISTEMAS \n",
       " \n",
       "ASESOR: \n",
       "Dr. RONY VILLAFUERTE SERNA \n",
       " \n",
       "CUSCO - PERÚ \n",
       "2024</b>\n",
       "\n",
       "<b>Respuesta: </b>\n",
       "Los autores son Br. Victor Abel Choquevilca Quispe y Br. Erika Alexandra Morales Valencia.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Quienes son los autores?\\n UNIVERSIDAD NACIONAL DE SAN ANTONIO ABAD DEL CUSCO \\n \\nFACULTAD DE INGENIERÍA ELÉCTRICA, ELECTRÓNICA, INFORMÁTICA Y \\nMECÁNICA \\nESCUELA PROFESIONAL DE INGENIERÍA INFORMÁTICA Y DE SISTEMAS \\n \\nTESIS \\n \\n \\n \\n \\n \\n \\nPRESENTADO POR: \\nBr. VICTOR ABEL CHOQUEVILCA QUISPE \\n \\nBr. ERIKA ALEXANDRA MORALES VALENCIA  \\n \\nPARA OPTAR EL TITULO PROFESIONAL DE \\nINGENIERO INFORMÁTICO Y DE SISTEMAS \\n \\nASESOR: \\nDr. RONY VILLAFUERTE SERNA \\n \\nCUSCO - PERÚ \\n2024\"\n",
    "result = llm(prompt.format(text=query))\n",
    "\n",
    "# Mostrar informacion completa de la consulta\n",
    "display(Markdown(f\"<p>{result}</p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ``embedding`` es una representación numérica de una información, por ejemplo, texto, documentos, imágenes, audio, etc. La representación captura el significado semántico de lo que se está incrustando, lo que la hace robusta para muchas aplicaciones de la industria.\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:550/1*ICGYEfQuwRUoadqbWrZLkw.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ``thenlper/gte-large`` es un modelo de embedding de texto que destaca por su eficacia y tamaño, pero está especializado en texto en ingles y se trunca en 512 tokens, mas info en: https://huggingface.co/thenlper/gte-large\n",
    "- ``hiiamsid/sentence_similarity_spanish_es`` es un modelo de embedding de texto especializado para texto en español, por lo que podria ser la mejor opcion, mas info en: https://huggingface.co/hiiamsid/sentence_similarity_spanish_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = 'hiiamsid/sentence_similarity_spanish_es' #thenlper/gte-large\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name = embedding_model,\n",
    "    model_kwargs = {\"device\": \"cuda\"},\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener datos\n",
    "En este ejemplo muy simple, estamos obteniendo datos historicos de la carrera de ingenieria Informática y de sistemas de la UNSAAC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''La escuela profesional de Ingenieria Informatica y de sistemas fue creado el 13 de Diciembre de 1971, \n",
    "dentro de la Aprobación del Plan de Estructuración del Programa Académico de Ciencias Fí­sico-Matemáticas según \n",
    "resolución Nº CG-110-71; luego, mediante resolución Nº CU-056-91 se aprueba el Proyecto para la Creación del \n",
    "Departamento Académico de Informática para ser elevado a Asamblea Universitaria y es Reaperturado el 22 de Enero \n",
    "de 1993 mediante resolución del Consejo Universitario Nº CU-009-93. Se aprueba el Proyecto de Acreditación de \n",
    "la Carrera Profesional de Ingenierí­a Informática y de Sistemas el 28 de noviembre del año 2013 por resolución \n",
    "Nº R-2193-2013-UNSAAC. Finalmente con nuevo Currí­culo de Estudios, con fecha 10 de septiembre del año 2014, \n",
    "es aprobado por resolución CU-232-2014-UNSAAC.'''\n",
    "document1 = Document(page_content=(text))\n",
    "\n",
    "text2 = '''Con la Ley Universitaria Nº 30220, Aprobado el Estatuto, la Asamblea Estatutaria asume funciones de la \n",
    "Asamblea Universitaria, instalándose el 14 de agosto del 2015 y mediante Resolución Nº 002-2015-AU-UNSAAC de 20 \n",
    "de agosto de 2015, se produce encargatura de Decanatos. En caso de la Carrera Profesional de Ingenierí­a Informática \n",
    "y de Sistemas, se convierte parte de la Facultad denominada 'Facultad de Ingenierí­a Eléctrica, Electrónica, Informática \n",
    "y de Sistemas' con nombre de 'Escuela Profesional de Ingenierí­a Informática y de Sistemas'.​'''\n",
    "document2 = Document(page_content=(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir el documento en fragmentos\n",
    "En un sistema RAG, es crucial dividir el documento en fragmentos (chunks) más pequeños para que sea más efectivo identificar y recuperar la información más relevante en el proceso de recuperación posterior. En este ejemplo, simplemente dividimos nuestro texto por caracteres, combinando 256 caracteres en cada fragmento. Donde se obtienen 7 chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=64)\n",
    "texts_chunks = text_splitter.split_documents([document1, document2])\n",
    "\n",
    "len(texts_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='La escuela profesional de Ingenieria Informatica y de sistemas fue creado el 13 de Diciembre de 1971, \\ndentro de la Aprobación del Plan de Estructuración del Programa Académico de Ciencias Fí\\xadsico-Matemáticas según'),\n",
       " Document(page_content='resolución Nº CG-110-71; luego, mediante resolución Nº CU-056-91 se aprueba el Proyecto para la Creación del \\nDepartamento Académico de Informática para ser elevado a Asamblea Universitaria y es Reaperturado el 22 de Enero'),\n",
       " Document(page_content='de 1993 mediante resolución del Consejo Universitario Nº CU-009-93. Se aprueba el Proyecto de Acreditación de \\nla Carrera Profesional de Ingenierí\\xada Informática y de Sistemas el 28 de noviembre del año 2013 por resolución'),\n",
       " Document(page_content='Nº R-2193-2013-UNSAAC. Finalmente con nuevo Currí\\xadculo de Estudios, con fecha 10 de septiembre del año 2014, \\nes aprobado por resolución CU-232-2014-UNSAAC.'),\n",
       " Document(page_content='Con la Ley Universitaria Nº 30220, Aprobado el Estatuto, la Asamblea Estatutaria asume funciones de la \\nAsamblea Universitaria, instalándose el 14 de agosto del 2015 y mediante Resolución Nº 002-2015-AU-UNSAAC de 20'),\n",
       " Document(page_content=\"de agosto de 2015, se produce encargatura de Decanatos. En caso de la Carrera Profesional de Ingenierí\\xada Informática \\ny de Sistemas, se convierte parte de la Facultad denominada 'Facultad de Ingenierí\\xada Eléctrica, Electrónica, Informática\"),\n",
       " Document(page_content=\"y de Sistemas' con nombre de 'Escuela Profesional de Ingenierí\\xada Informática y de Sistemas'.\\u200b\")]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consideraciones:\n",
    "\n",
    "1. **Tamaño de los fragmentos:** Dependiendo del caso de uso específico, puede ser necesario personalizar o experimentar con diferentes tamaños de fragmentos y solapamientos para lograr un rendimiento óptimo en RAG. Por ejemplo, los fragmentos (chunks) más pequeños pueden ser más beneficiosos en los procesos de recuperación, ya que los fragmentos de texto más grandes a menudo contienen texto de relleno que puede oscurecer la representación semántica. Por lo tanto, el uso de fragmentos de texto más pequeños en el proceso de recuperación puede permitir que el sistema RAG identifique y extraiga información relevante de manera más efectiva y precisa. Sin embargo, vale la pena considerar los compromisos que conlleva el uso de fragmentos más pequeños, como el aumento del tiempo de procesamiento y los recursos computacionales.\n",
    "\n",
    "2. **Cómo dividir:** Si bien el método más simple es dividir el texto por caracteres, existen otras opciones dependiendo del caso de uso y la estructura del documento. Por ejemplo, para evitar exceder los límites de tokens en las llamadas a la API, puede ser necesario dividir el texto por tokens. Para mantener la cohesión de los fragmentos, puede ser útil dividir el texto por oraciones, párrafos o encabezados HTML. Si trabajas con código, a menudo se recomienda dividir por fragmentos de código significativos, por ejemplo, utilizando un analizador de Árbol de Sintaxis Abstracta (AST).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear incrustaciones para cada fragmento de texto con Chroma\n",
    "Para cada fragmento de texto, luego necesitamos crear embeddings (incrustaciones) de texto, que son representaciones numéricas del texto en el espacio vectorial. Se espera que las palabras con significados similares estén en una proximidad más cercana o tengan una distancia más corta en el espacio vectorial. \n",
    "\n",
    "Para crear un embedding se utiliza la biblioteca ``Chroma`` para crear una base de datos de vectores a partir de fragmentos de texto (``texts_chunks``) y sus correspondientes ``embeddings``. La base de datos se guarda en el directorio especificado (``\"db\"``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if db != None:  # Re instanciar chroma\n",
    "        print(f\"Existe un base de datos Chroma, contiene {len(db.get()['documents'])} documentos, se eliminara.\")\n",
    "        db.delete_collection()\n",
    "except Exception as e:\n",
    "    print(\"No existe una base de datos.\")\n",
    "\n",
    "db = Chroma.from_documents(texts_chunks, embeddings)#, persist_directory=\"db\")\n",
    "print(f\"Se creó una base de datos Chroma con {len(db.get()['documents'])} elementos.\")\n",
    "db._collection.get(include=['embeddings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostrar los embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicacion de RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<b>Instrucciones: </b>\n",
    "<em> Utiliza los siguientes fragmentos de contexto para responder la pregunta al final.\n",
    "Si no sabes la respuesta, simplemente di que no lo sabes, no intentes inventar una respuesta.\n",
    "Utiliza un máximo de tres oraciones y mantén la respuesta lo más concisa posible. </em>\n",
    "\n",
    "<b>Contextos: </b><em>{context}</em>\\n\n",
    "<b>Pregunta: {question}</b>\n",
    "\n",
    "<b>Respuesta: </b>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = db.as_retriever(search_kwargs = {\"k\": 3}),\n",
    "    return_source_documents = True,\n",
    "    chain_type_kwargs = {\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<p><b>Instrucciones: </b>\n",
       "<em> Utiliza los siguientes fragmentos de contexto para responder la pregunta al final.\n",
       "Si no sabes la respuesta, simplemente di que no lo sabes, no intentes inventar una respuesta.\n",
       "Utiliza un máximo de tres oraciones y mantén la respuesta lo más concisa posible. </em>\n",
       "\n",
       "<b>Contextos: </b><em>Con la Ley Universitaria Nº 30220, Aprobado el Estatuto, la Asamblea Estatutaria asume funciones de la \n",
       "Asamblea Universitaria, instalándose el 14 de agosto del 2015 y mediante Resolución Nº 002-2015-AU-UNSAAC de 20\n",
       "\n",
       "de agosto de 2015, se produce encargatura de Decanatos. En caso de la Carrera Profesional de Ingenierí­a Informática \n",
       "y de Sistemas, se convierte parte de la Facultad denominada 'Facultad de Ingenierí­a Eléctrica, Electrónica, Informática\n",
       "\n",
       "La escuela profesional de Ingenieria Informatica y de sistemas fue creado el 13 de Diciembre de 1971, \n",
       "dentro de la Aprobación del Plan de Estructuración del Programa Académico de Ciencias Fí­sico-Matemáticas según</em>\n",
       "\n",
       "<b>Pregunta: En que provincia esta Sicuani?</b>\n",
       "\n",
       "<b>Respuesta: </b>\n",
       "No se proporciona información sobre la provincia en donde se encuentra Sicuani.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"En que provincia esta Sicuani?\"\n",
    "result_= qa_chain(query)\n",
    "result = result_[\"result\"].strip()\n",
    "\n",
    "display(Markdown(f\"<p>{result}</p>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<p><b>Instrucciones: </b>\n",
       "<em> Utiliza los siguientes fragmentos de contexto para responder la pregunta al final.\n",
       "Si no sabes la respuesta, simplemente di que no lo sabes, no intentes inventar una respuesta.\n",
       "Utiliza un máximo de tres oraciones y mantén la respuesta lo más concisa posible. </em>\n",
       "\n",
       "<b>Contextos: </b><em>La escuela profesional de Ingenieria Informatica y de sistemas fue creado el 13 de Diciembre de 1971, \n",
       "dentro de la Aprobación del Plan de Estructuración del Programa Académico de Ciencias Fí­sico-Matemáticas según\n",
       "\n",
       "de agosto de 2015, se produce encargatura de Decanatos. En caso de la Carrera Profesional de Ingenierí­a Informática \n",
       "y de Sistemas, se convierte parte de la Facultad denominada 'Facultad de Ingenierí­a Eléctrica, Electrónica, Informática\n",
       "\n",
       "y de Sistemas' con nombre de 'Escuela Profesional de Ingenierí­a Informática y de Sistemas'.​</em>\n",
       "\n",
       "<b>Pregunta: Cuando fue fundada la escuela profesional?</b>\n",
       "\n",
       "<b>Respuesta: </b>\n",
       "<ul>\n",
       "    <li>La escuela profesional de Ingeniería Informatica y de sistemas fue fundada en 1971.</li>\n",
       "</ul></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Cuando fue fundada la escuela profesional?\"\n",
    "result_= qa_chain(query)\n",
    "result = result_[\"result\"].strip()\n",
    "\n",
    "display(Markdown(f\"<p>{result}</p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de información de PDFS con RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de datos de un PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(\"tess.pdf\")\n",
    "data = loader.load()\n",
    "\n",
    "# Convertir todos los elementos de 'data' en objetos 'Document'\n",
    "documents = [Document(page_content=item.page_content) for item in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividir el documento en trozos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4096, chunk_overlap=64)\n",
    "texts_chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if db != None:  # Re instanciar chroma\n",
    "        print(f\"Existe un base de datos Chroma, contiene {len(db.get()['documents'])} documentos, se eliminara.\")\n",
    "        db.delete_collection()\n",
    "except Exception as e:\n",
    "    print(\"No existe una base de datos.\")\n",
    "\n",
    "db = Chroma.from_documents(texts_chunks, embeddings)#, persist_directory=\"db\")\n",
    "print(f\"Se creó una base de datos Chroma con {len(db.get()['documents'])} elementos.\")\n",
    "db._collection.get(include=['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<b>Instrucciones: </b>\n",
    "<em> Vas a analizar una tesis.\n",
    "Utiliza los siguientes fragmentos de contexto para responder la pregunta al final.\n",
    "Si no sabes la respuesta, simplemente di que no lo sabes, no intentes inventar una respuesta.\n",
    "Utiliza un máximo de tres oraciones y mantén la respuesta lo más concisa posible. </em>\n",
    "\n",
    "<b>Contextos: </b><em>{context}</em>\\n\n",
    "<b>Pregunta: {question}</b>\n",
    "\n",
    "<b>Respuesta: </b>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = db.as_retriever(search_kwargs = {\"k\": 3}),\n",
    "    return_source_documents = True,\n",
    "    chain_type_kwargs = {\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<p><b>Instrucciones: </b>\n",
       "<em> Vas a analizar una tesis.\n",
       "Utiliza los siguientes fragmentos de contexto para responder la pregunta al final.\n",
       "Si no sabes la respuesta, simplemente di que no lo sabes, no intentes inventar una respuesta.\n",
       "Utiliza un máximo de tres oraciones y mantén la respuesta lo más concisa posible. </em>\n",
       "\n",
       "<b>Contextos: </b><em>25 \n",
       " \n",
       "infracciones de tránsito en la Municipalidad Provincial del Cusco. Por lo tanto, es \n",
       "considerado como una investigación APLICADA. Siendo la definición: “La \n",
       "investigación Aplicada o Técnica tiende a la resolución de problemas o al desarrollo \n",
       "de ideas, dirigidas a conseguir innovaciones, mejoras de procesos o productos, etc.” \n",
       "(Sanchez, 2011). \n",
       "b) Por el propósito del estudio: El presente proyecto realizará la identificación de las \n",
       "características más sobresalientes de la implementación de un Servicio Web con \n",
       "Blockchain, destacando los aspectos más sobresalientes para la mejora en la seguridad \n",
       "de gestión de infracciones de tránsito en la Municipalidad Provincial del Cusco. Por lo \n",
       "tanto, es considerada como DESCRIPTIVA. Siendo la definición: “Los estudios \n",
       "descriptivos buscan especificar las propiedades, las características y los aspectos \n",
       "importantes del fenómeno que se somete a análisis” (Gomez, 2006). \n",
       " \n",
       "1.7.2 Metodología de desarrollo de software \n",
       "Se aplicará la metodología de desarrollo “Extreme Programming”, que es una \n",
       "metodología ágil muy exitosa porque hace hincapié en la satisfacción del cliente y \n",
       "permite a los desarrolladores responder con confianza a las necesidades cambiantes de \n",
       "los clientes, incluso al final del ciclo de vida. \n",
       "El Extreme Programming, que es un diagrama general que abarca todas las fases del \n",
       "proyecto, el cual tiene como inicio la administración, donde se encuentran las tareas \n",
       "necesarias de coordinación para la dotación del espacio de trabajo para el equipo en la \n",
       "organización y de supervisión de la aplicación en los parámetros de la metodología. A \n",
       "continuación, la fase de planeamiento en donde se recogen los requerimientos del\n",
       "\n",
       "24 \n",
       " \n",
       "mediante Servicios Web a la base de datos del Registro Nacional de Identificación y Estado \n",
       "Civil (RENIEC), de la Superintendencia Nacional de Registros Públicos (SUNARP) y del \n",
       "Ministerio de Transportes y Comunicaciones (MTC), de donde se obtendrán los datos de la \n",
       "persona, licencia de conducir y del vehículo. \n",
       " \n",
       "1.6 Limitaciones \n",
       "• \n",
       "Política de Reserva de Información: La Municipalidad Provincial del Cusco mantiene \n",
       "una política de reserva de información que restringe la divulgación completa de los \n",
       "datos almacenados en su base de datos. Esta política limita el acceso y la difusión de \n",
       "ciertos datos, lo que puede afectar la transparencia y la capacidad de realizar un \n",
       "análisis exhaustivo. \n",
       "• \n",
       "Actualización de Datos:  La actualización de los datos almacenados en la base de datos \n",
       "es un proceso que requiere tiempo. Dada la naturaleza progresiva de este proceso, que \n",
       "depende de la cantidad de datos que se agregan diariamente, la actualización completa \n",
       "de los datos con el sistema que se desarrollará puede llevar un tiempo considerable. \n",
       " \n",
       "1.7 Metodología \n",
       "La metodología de este proyecto se divide en dos componentes principales: la metodología \n",
       "de investigación y la metodología de desarrollo de software. \n",
       "1.7.1 Metodología de investigación \n",
       "La metodología de investigación se clasifica de la siguiente manera: \n",
       "a) Por la forma en que la investigación es usada: El siguiente proyecto desea dar \n",
       "solución a los problemas que presenta la falta de seguridad en la gestión de\n",
       "\n",
       "28 \n",
       " \n",
       "sistema implementado así como también nos describe la implementación de la API, la \n",
       "cual se realizó mediante consultas POST al servidor que permite interactuar con el \n",
       "Blockchain a través de una serie de rutas. \n",
       "➢   Espíritu Aranda, Walter Augusto y Machuca Nieva, Christian Fernando (2021) “Modelo   \n",
       "de Referencia para la Gestión de la Seguridad de Datos de Salud soportado en una \n",
       "Plataforma Blockchain” (Para optar el título profesional de Ingeniero de Sistemas de   \n",
       "Información) Universidad Peruana de Ciencias Aplicadas (Espíritu Aranda & Machuca \n",
       "Nieva, 2021). \n",
       "Conclusiones: \n",
       "• Los resultados obtenidos indican que los costos de implementar controles mitigantes \n",
       "en los centros de salud son elevados a comparación de utilizar la tecnología \n",
       "Blockchain la cual minimiza en su mayoría las brechas de seguridad, con relación a \n",
       "la cantidad de riesgos y vulnerabilidades encontrados en los sistemas de la clínica \n",
       "que albergan los datos de salud de pacientes. \n",
       "• Con la implementación del modelo de referencia, los centros de salud tienen una \n",
       "visión detallada y específica acerca de los posibles riesgos que podrían ocurrir, \n",
       "evitando problemas legales o sanciones económicas por parte del ente regulador. \n",
       "• El uso del modelo de referencia tiene un impacto positivo para la gestión de la \n",
       "seguridad en los centros de salud debido a que permite realizar un diagnóstico sobre \n",
       "los activos de información que tiene como objetivo conocer la criticidad de cada uno \n",
       "de ellos.</em>\n",
       "\n",
       "<b>Pregunta: cuales son las metodologias de investigacion que se utilizan?</b>\n",
       "\n",
       "<b>Respuesta: </b>\n",
       "\n",
       "<p>Las metodologías de investigación que se utilizan son:</p>\n",
       "\n",
       "<ol>\n",
       "<li>Descriptiva</li>\n",
       "<li>Aplicada</li>\n",
       "</ol></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"cuales son las metodologias de investigacion que se utilizan?\"\n",
    "result_= qa_chain(query)\n",
    "result = result_[\"result\"].strip()\n",
    "\n",
    "display(Markdown(f\"<p>{result}</p>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<p><b>Instrucciones: </b>\n",
       "<em> Vas a analizar una tesis.\n",
       "Utiliza los siguientes fragmentos de contexto para responder la pregunta al final.\n",
       "Si no sabes la respuesta, simplemente di que no lo sabes, no intentes inventar una respuesta.\n",
       "Utiliza un máximo de tres oraciones y mantén la respuesta lo más concisa posible. </em>\n",
       "\n",
       "<b>Contextos: </b><em>UNIVERSIDAD NACIONAL DE SAN ANTONIO ABAD DEL CUSCO \n",
       " \n",
       "FACULTAD DE INGENIERÍA ELÉCTRICA, ELECTRÓNICA, INFORMÁTICA Y \n",
       "MECÁNICA \n",
       "ESCUELA PROFESIONAL DE INGENIERÍA INFORMÁTICA Y DE SISTEMAS \n",
       " \n",
       "TESIS \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       "PRESENTADO POR: \n",
       "Br. VICTOR ABEL CHOQUEVILCA QUISPE \n",
       " \n",
       "Br. ERIKA ALEXANDRA MORALES VALENCIA  \n",
       " \n",
       "PARA OPTAR EL TITULO PROFESIONAL DE \n",
       "INGENIERO INFORMÁTICO Y DE SISTEMAS \n",
       " \n",
       "ASESOR: \n",
       "Dr. RONY VILLAFUERTE SERNA \n",
       " \n",
       "CUSCO - PERÚ \n",
       "2024 \n",
       "BLOCKCHAIN APLICADO A LA SEGURIDAD PARA LA GESTIÓN DE \n",
       "INFRACCIONES DE TRÁNSITO EN LA MUNICIPALIDAD PROVINCIAL \n",
       "DEL CUSCO\n",
       "\n",
       "31 \n",
       " \n",
       "computacional que le toma al sistema procesar el algoritmo. Este último punto, es \n",
       "fundamental considerarlo ya que debido a que mientras más procesamiento \n",
       "computacional se realice más monedas electrónicas se utiliza en el sistema y esto \n",
       "puede incrementar en gran manera los costos del uso del sistema. \n",
       "• El tercer objetivo específico fue utilizar estándares legales y técnicos en las fases de \n",
       "emisión, escrutinio y auditoría. En tal sentido, un resultado alcanzado fundamental \n",
       "fue la creación del catálogo de requerimientos, el cual constituye la base para la \n",
       "definición del nivel de seguridad que posee el sistema. Es por ello, que el catálogo de \n",
       "requerimientos sufrió tres versiones a lo largo del proyecto de tesis. Así mismo, se \n",
       "implementó el módulo de verificación individual de los votos y funcionalidades que \n",
       "permitan auditar el sistema. Con estas últimas características se pudo incrementar y \n",
       "finalizar el nivel de seguridad del sistema propuesto. En este objetivo más enfocado \n",
       "al control de cambios registrados en el sistema lo que permitían un nivel alto de \n",
       "auditoría del sistema por parte del elector como por parte del auditor.  \n",
       "• Finalmente, el gobierno electrónico en el cual se implementó el sistema fue las \n",
       "elecciones generales para procesos electores en el Perú. En tal sentido, el alcance del \n",
       "proyecto es a nivel nacional; sin embargo, es factible poder implementarlo en un \n",
       "gobierno electrónico distrital, donde se debería tener en consideración el lugar de \n",
       "residencia del elector, o en un nivel de gobierno electrónico de colegio de \n",
       "profesionales en el Perú en donde el uso de sistema de voto electrónico no \n",
       "presencial es más utilizado. Inclusive se podría agregar mayor nivel en la fase de \n",
       "configuración del proceso electoral para aceptar otros tipos de procesos electorales, \n",
       "tales como el referéndum.\n",
       "\n",
       "72 \n",
       " \n",
       "CAPÍTULO IV \n",
       "IMPLEMENTACION DEL BLOCKCHAIN \n",
       "4.1 Diseño del modelo \n",
       "Representando diferentes modelos y arquitecturas técnicas gubernamentales, se realizó el \n",
       "análisis para el manejo de las infracciones de tránsito como contexto y técnica. Siendo así se \n",
       "desarrolló el siguiente modelo técnico.   \n",
       "  \n",
       "Componentes del modelo \n",
       "Datos del Infractor\n",
       "Datos del Vehiculo\n",
       "Datos \n",
       "Complementarios\n",
       "Infracción \n",
       "de Transito\n",
       "Reportes\n",
       "Fase de Ingreso\n",
       "Fase de control\n",
       "Verificación\n",
       "Creación\n",
       "Consultas\n",
       "Proceso\n",
       "Conservación\n",
       "Entrada\n",
       "Gestión de Infracciones\n",
       "Salida\n",
       "Tecnologia y Seguridad\n",
       "Autenticación\n",
       "Cadena de Bloques\n",
       " \n",
       "El modelo planteado en la Figura 14 se divide en 4 componentes, que permite la gestión de \n",
       "infracciones de tránsito. \n",
       "A continuación, se detalla cada uno de los componentes del modelo. \n",
       "4.1.1 Entrada. \n",
       "En este componente se detalla la forma de ingreso de los datos al modelo tecnológico \n",
       "con relación a la capa de gestión de infracciones.</em>\n",
       "\n",
       "<b>Pregunta: quienes son PARA OPTAR EL TITULO PROFESIONAL DE INGENIERO INFORMÁTICO Y DE SISTEMAS</b>\n",
       "\n",
       "<b>Respuesta: </b>\n",
       "\n",
       "<ul>\n",
       "<li>Br. Victor Abel Chuquevilca Quispe</li>\n",
       "<li>Br. Erika Alexandra Morales Valencian</li>\n",
       "</ul></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"quienes son PARA OPTAR EL TITULO PROFESIONAL DE INGENIERO INFORMÁTICO Y DE SISTEMAS\"\n",
    "result_= qa_chain(query)\n",
    "result = result_[\"result\"].strip()\n",
    "\n",
    "display(Markdown(f\"<p>{result}</p>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<p><b>Instrucciones: </b>\n",
       "<em> Vas a analizar una tesis.\n",
       "Utiliza los siguientes fragmentos de contexto para responder la pregunta al final.\n",
       "Si no sabes la respuesta, simplemente di que no lo sabes, no intentes inventar una respuesta.\n",
       "Utiliza un máximo de tres oraciones y mantén la respuesta lo más concisa posible. </em>\n",
       "\n",
       "<b>Contextos: </b><em>AGRADECIMIENTO \n",
       " \n",
       "A nuestro asesor Dr. Rony Villafuerte Serna por el tiempo, conocimiento y apoyo profesional \n",
       "que nos brindó durante todo el proceso de investigación para la culminación de este proyecto. \n",
       " \n",
       "A nuestros dictaminantes, el Ing. Robert Wilbert Alzamora Paredes y el Ing. José Mauro Pillco \n",
       "Quispe, los cuales gentilmente, nos brindaron aportes para nuestro proyecto. \n",
       " \n",
       "A nuestros profesores, por todo el conocimiento y aportes que nos brindaron durante los años \n",
       "que estuvimos estudiando en la Universidad Nacional San Antonio Abad del Cusco. \n",
       " \n",
       "A la Gerencia de Tránsito, Viabilidad y Transito de la Municipal Provincial del Cusco por su \n",
       "valioso tiempo y apoyo hacia nuestro proyecto.\n",
       "\n",
       "29 \n",
       " \n",
       "• Los resultados obtenidos indican que hay una disminución de un 26% en el nivel de \n",
       "riesgo con el uso de la tecnología Blockchain a comparación de usar un sistema \n",
       "tradicional. \n",
       "Comentario: \n",
       "En este proyecto de investigación proponen un modelo referencial para ser utilizados en \n",
       "instituciones de salud, ya que permite la gestión de la seguridad de datos sensibles y \n",
       "confidenciales apoyado en una plataforma Blockchain que integra el estándar ISO/IEC \n",
       "27799, así mismo, mediante una evaluación del riesgo concluyen que el uso de la \n",
       "tecnología Blockchain disminuye el nivel del riesgo. \n",
       "➢ Sebastián Andrés Sánchez Herrera (2021) “Sistema de voto electrónico basado en \n",
       "Blockchain” (Tesis Para optar por el Título de Ingeniero Informático) Pontificia \n",
       "Universidad Católica del Perú, Lima Perú  (Sanchez Herrera, 2021). \n",
       "Conclusiones: \n",
       "• El primer objetivo específico fue implementar un sistema de voto electrónico de \n",
       "código abierto que gestione la información del proceso electoral de forma \n",
       "descentralizada para los actores del proceso electoral. En base a la arquitectura \n",
       "diseñada resultante de las fases de análisis y diseño, se consiguió implementar un \n",
       "sistema de voto electrónico para elecciones generales en el Perú compuesto por una \n",
       "capa front-end en React Js y una capa de back-end en la tecnología Blockchain con \n",
       "un servicio de envío de correos desarrollado en Spring. Una vez escogida la \n",
       "arquitectura del sistema a utilizar, se definieron tres módulos que permitieron \n",
       "conseguir este primer objetivo específico. Estos tres módulos fueron el módulo de \n",
       "emisión de votos, escrutinio de los votos y mantener un repositorio del proyecto con\n",
       "\n",
       "UNIVERSIDAD NACIONAL DE SAN ANTONIO ABAD DEL CUSCO \n",
       " \n",
       "FACULTAD DE INGENIERÍA ELÉCTRICA, ELECTRÓNICA, INFORMÁTICA Y \n",
       "MECÁNICA \n",
       "ESCUELA PROFESIONAL DE INGENIERÍA INFORMÁTICA Y DE SISTEMAS \n",
       " \n",
       "TESIS \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       "PRESENTADO POR: \n",
       "Br. VICTOR ABEL CHOQUEVILCA QUISPE \n",
       " \n",
       "Br. ERIKA ALEXANDRA MORALES VALENCIA  \n",
       " \n",
       "PARA OPTAR EL TITULO PROFESIONAL DE \n",
       "INGENIERO INFORMÁTICO Y DE SISTEMAS \n",
       " \n",
       "ASESOR: \n",
       "Dr. RONY VILLAFUERTE SERNA \n",
       " \n",
       "CUSCO - PERÚ \n",
       "2024 \n",
       "BLOCKCHAIN APLICADO A LA SEGURIDAD PARA LA GESTIÓN DE \n",
       "INFRACCIONES DE TRÁNSITO EN LA MUNICIPALIDAD PROVINCIAL \n",
       "DEL CUSCO</em>\n",
       "\n",
       "<b>Pregunta: Quienes son los dictaminantes de la tesis?</b>\n",
       "\n",
       "<b>Respuesta: </b>\n",
       "Los dictaminadores de la tesis son el Ing. Robert Wilbert Alzamora Paredes y el Ing. José Mauro Pillco Quispe.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Quienes son los dictaminantes de la tesis?\"\n",
    "result_= qa_chain(query)\n",
    "result = result_[\"result\"].strip()\n",
    "\n",
    "display(Markdown(f\"<p>{result}</p>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
